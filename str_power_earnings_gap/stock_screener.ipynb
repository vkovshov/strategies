{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-13 14:11:31.822816] INFO: Norgate Data: NorgateData package v1.0.74: Init complete\n",
      "connected to: dbmaster\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import func\n",
    "\n",
    "sys.path.append(os.path.abspath('../../fin_data'))\n",
    "from utils.date_functions import last_business_day\n",
    "from utils.helper_functions import get_test_universe_tickers\n",
    "from utils.postgresql_conn import get_session\n",
    "from utils.postgresql_tables import Company, Tickers, HistoricalPrice\n",
    "from utils.postgresql_data_query import get_effective_dates, get_company_reports_in_period\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "def create_universe_df(session, tickers, earnings_dict):\n",
    "    \n",
    "    company_id_mapping = session.query(Company.ticker, Company.id).filter(Company.ticker.in_(tickers)).all()\n",
    "    ticker_to_company_id = {ticker: company_id for ticker, company_id in company_id_mapping}\n",
    "    \n",
    "    ticker_id_mapping = session.query(Tickers.ticker, Tickers.id).filter(Tickers.ticker.in_(tickers)).all()\n",
    "    ticker_to_ticker_id = {ticker: ticker_id for ticker, ticker_id in ticker_id_mapping}\n",
    "\n",
    "    data = []\n",
    "    for ticker in tickers:\n",
    "        company_id = ticker_to_company_id.get(ticker)\n",
    "        ticker_id = ticker_to_ticker_id.get(ticker)\n",
    "        if company_id and company_id in earnings_dict:\n",
    "            report_date = earnings_dict[company_id]\n",
    "            data.append([ticker, company_id, ticker_id, report_date])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['ticker', 'company_id', 'ticker_id','report_date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_price_data(session, ticker_dates_dict):\n",
    "    \"\"\"\n",
    "    Queries the historical_price table for price data for 3 days: the most recent day before, the day of, \n",
    "    and the first available day after the report_date, if available.\n",
    "    \n",
    "    Args:\n",
    "    - session: Database session object.\n",
    "    - ticker_dates_dict: Dictionary where key is ticker_id and value is the report_date.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns ['ticker_id', 'date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    \"\"\"\n",
    "    \n",
    "    all_price_data = []\n",
    "    \n",
    "    for ticker_id, report_date in ticker_dates_dict.items():\n",
    "        \n",
    "        # Get the most recent trading day before the report_date\n",
    "        day_before = (session.query(HistoricalPrice.date)\n",
    "                      .filter(HistoricalPrice.ticker_id == ticker_id)\n",
    "                      .filter(HistoricalPrice.date < report_date)\n",
    "                      .order_by(HistoricalPrice.date.desc())\n",
    "                      .first())\n",
    "        \n",
    "        # Get the first trading day after the report_date (extended to handle non-immediate days)\n",
    "        day_after = (session.query(HistoricalPrice.date)\n",
    "                     .filter(HistoricalPrice.ticker_id == ticker_id)\n",
    "                     .filter(HistoricalPrice.date > report_date)\n",
    "                     .order_by(HistoricalPrice.date.asc())\n",
    "                     .first())\n",
    "        \n",
    "        # Prepare list of dates to query\n",
    "        dates_to_query = [report_date]\n",
    "        \n",
    "        # Add day_before if it exists\n",
    "        if day_before:\n",
    "            dates_to_query.insert(0, day_before[0])\n",
    "        \n",
    "        # Add day_after if it exists\n",
    "        if day_after:\n",
    "            dates_to_query.append(day_after[0])\n",
    "        \n",
    "        # Get price data for the available days\n",
    "        price_data = (session.query(HistoricalPrice.ticker_id, \n",
    "                                    HistoricalPrice.date, \n",
    "                                    HistoricalPrice.open, \n",
    "                                    HistoricalPrice.high, \n",
    "                                    HistoricalPrice.low, \n",
    "                                    HistoricalPrice.close, \n",
    "                                    HistoricalPrice.volume)\n",
    "                      .filter(HistoricalPrice.ticker_id == ticker_id)\n",
    "                      .filter(HistoricalPrice.date.in_(dates_to_query))\n",
    "                      .all())\n",
    "        \n",
    "        # Extend the results to the final list\n",
    "        all_price_data.extend(price_data)\n",
    "    \n",
    "    # Convert the result to a DataFrame\n",
    "    df_price = pd.DataFrame(all_price_data, columns=['ticker_id', 'date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    \n",
    "    return df_price\n",
    "\n",
    "def get_volume_stats(session, ticker_dates_dict, lookback=20):\n",
    "    \"\"\"\n",
    "    Queries the last 'lookback' trading days before the report date and calculates volume stats (mean and std dev).\n",
    "    \n",
    "    Args:\n",
    "    - session: Database session object.\n",
    "    - ticker_dates_dict: Dictionary where key is ticker_id and value is the report_date.\n",
    "    - lookback: Number of trading days to look back for volume stats (default is 20).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with ticker_id, report_date, rolling_volume_mean, and rolling_volume_std.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_volume_stats = []\n",
    "\n",
    "    for ticker_id, report_date in ticker_dates_dict.items():\n",
    "        # Step 1: Query volume data for the 20 trading days before the report date\n",
    "        # Adding a buffer of 10 extra days to handle missing or non-trading days\n",
    "        volume_data = (session.query(HistoricalPrice.ticker_id,\n",
    "                                     HistoricalPrice.date,\n",
    "                                     HistoricalPrice.volume)\n",
    "                       .filter(HistoricalPrice.ticker_id == ticker_id)\n",
    "                       .filter(HistoricalPrice.date < report_date)  # Only fetch dates before the report\n",
    "                       .order_by(HistoricalPrice.date.desc())  # Most recent first\n",
    "                       .limit(lookback + 10)  # Fetch extra days to handle weekends and holidays\n",
    "                       .all())\n",
    "        \n",
    "        # Convert the result to a DataFrame\n",
    "        df_volume = pd.DataFrame(volume_data, columns=['ticker_id', 'date', 'volume'])\n",
    "        \n",
    "        # Ensure the dates are sorted in ascending order for rolling calculations\n",
    "        df_volume = df_volume.sort_values(by='date').reset_index(drop=True)\n",
    "        \n",
    "        # Step 2: Calculate rolling mean and std deviation of volume for the last 'lookback' days\n",
    "        df_volume['rolling_volume_mean'] = df_volume['volume'].rolling(window=lookback, min_periods=1).mean()\n",
    "        df_volume['rolling_volume_std'] = df_volume['volume'].rolling(window=lookback, min_periods=1).std()\n",
    "        \n",
    "        # Step 3: Get the last row's rolling statistics, as it represents the stats just before the report date\n",
    "        last_row = df_volume.iloc[-1]\n",
    "        volume_stats = {\n",
    "            'ticker_id': ticker_id,\n",
    "            'report_date': report_date,\n",
    "            'rolling_volume_mean': last_row['rolling_volume_mean'],\n",
    "            'rolling_volume_std': last_row['rolling_volume_std']\n",
    "        }\n",
    "        \n",
    "        # Append the stats for this ticker\n",
    "        all_volume_stats.append(volume_stats)\n",
    "    \n",
    "    # Convert the stats to a DataFrame\n",
    "    df_volume_stats = pd.DataFrame(all_volume_stats)\n",
    "    \n",
    "    return df_volume_stats\n",
    "\n",
    "def merge_price_data(df_universe, df_price, df_volume_stats, filter_gap=10):\n",
    "    \"\"\"\n",
    "    Merges the price data with the universe DataFrame based on ticker_id and report_date,\n",
    "    and incorporates volume stats to calculate the volume spike as a z-score.\n",
    "    \n",
    "    Args:\n",
    "    - df_universe: DataFrame containing the tickers, company_ids, ticker_ids, and report_dates.\n",
    "    - df_price: DataFrame with historical price data sorted by ticker_id and date.\n",
    "    - df_volume_stats: DataFrame with rolling volume stats (mean and std dev) from get_volume_stats.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with calculated gap, follow-through, and volume spike z-score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the date columns are in datetime format\n",
    "    df_universe['report_date'] = pd.to_datetime(df_universe['report_date'])\n",
    "    df_price['date'] = pd.to_datetime(df_price['date'])\n",
    "    df_volume_stats['report_date'] = pd.to_datetime(df_volume_stats['report_date'])\n",
    "    \n",
    "    # Sort price data by ticker_id and date\n",
    "    df_price = df_price.sort_values(by=['ticker_id', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Merge price data with df_universe\n",
    "    df_merged = pd.merge(df_universe, df_price, left_on=['ticker_id', 'report_date'], right_on=['ticker_id', 'date'], how='left')\n",
    "    \n",
    "    # Shift to get the previous day's close, next day's close, and next day's volume within each ticker_id group\n",
    "    df_price['prev_close'] = df_price.groupby('ticker_id')['close'].shift(1)\n",
    "    df_price['next_close'] = df_price.groupby('ticker_id')['close'].shift(-1)\n",
    "    df_price['next_volume'] = df_price.groupby('ticker_id')['volume'].shift(-1)\n",
    "    \n",
    "    # Merge previous close and next close/volume with df_merged\n",
    "    df_merged = pd.merge(df_merged, df_price[['ticker_id', 'date', 'prev_close']], \n",
    "                         left_on=['ticker_id', 'report_date'], right_on=['ticker_id', 'date'], how='left', suffixes=('', '_prev'))\n",
    "    \n",
    "    df_merged = pd.merge(df_merged, df_price[['ticker_id', 'date', 'next_close', 'next_volume']], \n",
    "                         left_on=['ticker_id', 'report_date'], right_on=['ticker_id', 'date'], how='left', suffixes=('', '_next'))\n",
    "    \n",
    "    # Drop unnecessary duplicate columns\n",
    "    df_merged.drop(columns=['date', 'date_prev', 'date_next'], inplace=True)\n",
    "    \n",
    "    # Step 1: Merge volume statistics (rolling mean and std) with the price data\n",
    "    df_merged = pd.merge(df_merged, df_volume_stats, on=['ticker_id', 'report_date'], how='left')\n",
    "    \n",
    "    # Step 2: Calculate gap as a percentage: (open on report day - close on prior day) / close on prior day\n",
    "    df_merged['gap'] = (df_merged['open'] - df_merged['prev_close']) / df_merged['prev_close'] * 100\n",
    "    \n",
    "    # Step 3: Calculate follow-through as a percentage: (close on next day - open on report day) / open on report day\n",
    "    df_merged['followthrough'] = (df_merged['next_close'] - df_merged['open']) / df_merged['open'] * 100\n",
    "    \n",
    "    # Step 4: Calculate volume spike z-score: (next_volume - rolling mean) / rolling std\n",
    "    df_merged['volume_zscore'] = round((df_merged['next_volume'] - df_merged['rolling_volume_mean']) / df_merged['rolling_volume_std'], 2).astype(str) + ' sigma'\n",
    "\n",
    "    # Step 5: Convert 'gap' column to numeric for filtering and apply the filter\n",
    "    df_filtered = df_merged[(df_merged['gap'].abs() + df_merged['followthrough'].abs()) >= filter_gap]  # Filter by absolute gap >= 5%\n",
    "\n",
    "    # Step 6: Convert 'gap' and 'followthrough' back to strings with percentages using .loc[]\n",
    "    df_filtered.loc[:, 'gap'] = df_filtered['gap'].round(2).astype(str) + '%'\n",
    "    df_filtered.loc[:, 'followthrough'] = df_filtered['followthrough'].round(2).astype(str) + '%'\n",
    "\n",
    "    # Step 7: Drop unnecessary columns using .loc[]\n",
    "    df_filtered = df_filtered.drop(columns=['open', 'high', 'low', 'volume', 'next_volume', \n",
    "                              'rolling_volume_mean', 'rolling_volume_std']).reset_index(drop=True)\n",
    "    \n",
    "    # Step 8: Select the final columns to display\n",
    "    df_filtered = df_filtered[['ticker', 'company_id', 'report_date', 'prev_close', \n",
    "                               'close', 'next_close', 'gap', 'followthrough', 'volume_zscore']]\n",
    "    \n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical range: 2024-11-12 to 2024-10-15\n"
     ]
    }
   ],
   "source": [
    "## set up parameters\n",
    "tickers = []\n",
    "reporting_currency = None #'USD'\n",
    "cols = ['Date','Open','High','Low','Close','Volume']\n",
    "path = '/Users/VadimKovshov/Dropbox/INVESTMENTS/EVALUTE/STOCKS/MODEL_OUTPUTS/POWER_EARNINGS_GAP/'\n",
    "\n",
    "w_offset = 0\n",
    "d_offset = max(1, w_offset * 7)\n",
    "end = last_business_day(offset=d_offset)\n",
    "start = last_business_day(offset=d_offset + 28)\n",
    "current_date = dt.date.today()\n",
    "print(f'Historical range: {end.strftime(\"%Y-%m-%d\")} to {start.strftime(\"%Y-%m-%d\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to: dbmaster\n",
      "Universe dates: 2024-11-08, 2024-11-01\n",
      "Universe tickers: 1760\n"
     ]
    }
   ],
   "source": [
    "# get universe dates, tickers & id's\n",
    "session = get_session()\n",
    "eff_date, pr_date = get_effective_dates(offset_0=w_offset)\n",
    "print(f'Universe dates: {eff_date.strftime(\"%Y-%m-%d\")}, {pr_date.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "for date in [eff_date, pr_date]:\n",
    "    tickers.extend(get_test_universe_tickers(session, date, currency_reporting=reporting_currency))\n",
    "tickers = list(set(tickers))\n",
    "print(f'Universe tickers: {len(tickers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies reported: 1351\n"
     ]
    }
   ],
   "source": [
    "company_reports = get_company_reports_in_period(session, start_date=start, end_date=end, dimension='arq')\n",
    "df_universe = create_universe_df(session, tickers=tickers, earnings_dict=company_reports) \\\n",
    "                                .sort_values(by=['report_date','ticker'], ascending=[False, True]) \\\n",
    "                                .reset_index(drop=True)\n",
    "ticker_dates_dict = dict(zip(df_universe['ticker_id'], df_universe['report_date']))\n",
    "print(f'Companies reported: {len(df_universe)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain & merge price data with universe data, volume data and calculate the gap and follow-through\n",
    "df_price = get_price_data(session, ticker_dates_dict=ticker_dates_dict)\n",
    "df_volume_stats = get_volume_stats(session, ticker_dates_dict=ticker_dates_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_price_data(df_universe=df_universe, df_price=df_price, df_volume_stats=df_volume_stats, filter_gap=10)\n",
    "df_merged.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(f'{path}power_earnings_gap_{current_date.strftime(\"%Y%m%d\")}.csv', index=False)\n",
    "df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
